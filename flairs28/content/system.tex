\section{\system}

\ceg{STILL NEED TO REARRANGE PARAGRAPH AND REWORD}

\cpg{Need a better introduction?}

\system is a web-based system build on top of the PostgreSQL open 
source database system. Computation for visualization is performed
using HTML, CSS and javascript. Calculations are performed using 
in-database computation through AJAX calls and client side 
javascript. In this section we discuss each part of \system. 

\subsection{Data pre-processing and topic learning}

Here we describe the main pre-processing steps we perform on a 
collection of articles for topic modeling and search. First, 
we tokenize raw texts of articles with the help of the python 
Natural Language Toolkit 
(NLTK)\footnote{\texttt{http://www.nltk.org/}} and a set of 
predefined regular expressions. Second, we standardize tokens by 
removing noise and stop-words. We use typical standardization 
techniques for word tokens such as \textsl{stemming}---for this 
project, we use the popular Porter stemming algorithm 
\cite{Porter1980} implementation in NLTK. Third, we represent each 
document in a sparse ``bag of words'' format, after building a 
vocabulary of corpus words. Last, we use them as input to the topic 
learning algorithm~\cite{hoffman2010online} which will in turn learn 
the latent topic structure of a corpus from the term co-occurrence 
frequencies of the corresponding documents. Components of a learned 
topic model includes the estimated corpus-level topics, $\beta_j^{*}$s, 
and document-level topic mixtures, $\theta_d^{*}$s. As discussed, 
$\beta_j^{*}$ is useful for our automatic detection of topics among  
articles. Similarly, $\theta_d^{*}$ give an idea of the topicality 
of a particular article given a topic. This is quite useful in 
finding similar articles and grouping them together. In addition, 
topic modeling is a type of dimensionality reduction technique that 
enables us to work on the topic-space rather than on the 
vocabulary-space.



\subsection{User Model}
When performing search, exploration and discovery over academic papers users 
may bring particular context to their search. Incorporating this information
into the search process has been show to be beneficial to 
users~\cite{DZSRWJ,MZPGSOL}.
We develop a user model that 
encapsulates the users personal context and integrates it into their
search task. 

This model is a distribution of weights for each topic.
Formally, given a set of topics $T$ the user model is defined as
$$
\mathcal{U} = \{u_0, \ldots, u_{|T|}\}
$$
where $u_i \in [0,1]$ and $\sum_{t \in T} u_t = 1$.
We graphically allow the user to select the weights that correspond to
each topic. This allows the users to change preferences with each query
for more desirable results.

The user model is used in different ways to provide better feedback to
the user. After a keyword search, the document results of the search 
are re-ranked by calculating the KL-divergence of each document and the
user model. Formally, given the set of result documents $D$:

\begin{equation} \label{eq:KL}
KL(\mathcal{U}||d) = \sum_{t \in T} u_t \ln \frac{u_t}{d_t}.
\end{equation}
where $d \in D$ and $d_t$ is the topic proportion for document $d$ and
topic $t \in T$. 

In the topic explorer, each topic row is color-coded like a heat 
based map based on the similarity of the user model to that topic (see Figure~\ref{fig:topic_exploration}).
The user can look at this heat map to adjust their topic preferences.
We use equation~\ref{eq:KL} on the client side to calculate this preference. 
In the graph explorer the citations for the current paper is ranked
using equation~\ref{eq:KL}. The citations of that paper that are most
similar to the user model are ranked the highest.


\subsection{Topic-Based Search and Exploration}

\system provides several ranking functions to let the user find  
the best articles. One way of ranking is to identify the topics of 
real interest, by looking at the most probable words of the estimated 
topics $\beta_j^{*}$ for the corpus, and then determine relevant 
articles given the topic of interest. In the next section, we 
describe how \system helps users visualizing the estimated topics. 
We exploit estimated topic distributions $\theta_d^{*}$s of 
individual articles, to rank them on relevance given a topic. Let 
$t$ be the index for the topic of interest, we calculate
\cite{George2012}
\begin{equation}
m(d) = \ln \theta^*_{dt} + \sum_{j \neq t}{\ln (1 - \theta^*_{dj})}
\end{equation}
for all documents $d = 1, 2, \ldots, D$ in the document collection, 
where $j = 1, 2, \ldots, K$, and sort them to rank them on relevance. 
Here, we assume that each $\theta^*_{dt}$ is normalized, i.e., 
$\sum_{j=1}^{K}{\theta^*_{dj}} = 1$. Intuitively, we can see that 
this equation will give a high value for a document, if the probability 
of occurring topic $t$ is high in that document. This means a 
document with a higher value of this score is highly relevant for 
the topic of interest $t$, and contains a considerable amount of 
words from topic $t$. The next section describes a visualization 
scheme of the ranked documents given the estimated set of topics.      


\noindent\textbf{Topic-Based Exploration}

\ceg{clint stuff}

\noindent\textbf{Lineage search}


%\subsection{Visualization}
%At times it is important to allow the user to do exploritory search.

