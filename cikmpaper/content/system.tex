\section{\system}

In this section we discuss each part of \system.

\subsection{Data pre-processing and topic learning}
First, we tokenize the raw paper texts based on the python NLTK toolkit and a
predefined regular expression. Second, we standardize them by removing noise terms
and stop-words. Third, we represent each document in a sparse 
bag-of-words format, after building a vocabulary of
corpus-words. Last, we use them as input to the topic learning model which will
in turn learn clusters from the term co-occurrence frequencies of the corresponding
documents. In this project, we used the Gensim 
package implementation of the LDA online learning algorithm 
\cite{rehurek_lrec, hoffman2010online} for topic learning, which is 
based on the variational inference framework.    

Components of a learned topic model includes the corpus-level topic word association counts
and document-level topic mixtures. Each estimated topic is represented
by its topic-word-counts, which is useful for our automatic detection of paper topics. 
The document-level topic mixtures give an idea of the topicality of a
particular paper given a topic. This is also quite useful in finding similar papers and
grouping them together, because topic modeling is a type of dimensionality reduction
technique that enables us to work on the topic-space rather than on 
the vocabulary-space.




\subsection{User Model}
A user gives their abstraction of topic preferences.


\subsection{Ranking Function}
We provided several ranking functions to let the user find
the best papers.

One way to rank is to determine relevant papers 
given an estimated topic. We use individual papers'
document topic mixtures, $\hat{\theta_d}$, to rank them 
on relevance given a topic. For a given 
topic $t \in K$, we calculate
\begin{equation}
m(d) = \ln \hat{\theta}_{d,t} + \sum_{j \neq t}{\ln (1 - \hat{\theta}_{d,j})}
\end{equation}
for all documents $d = 1, 2, ..., D$ in 
the document collection, and sort them to rank
them on relevance. Here, we assume that
each $\hat{\theta}_d$ is normalized, 
i.e., $\sum_{j=1}^{K}{\hat{\theta}_j} = 1$.
Intuitively, we can see that this equation maximizes the probability
of a topic $t \in K$ given a document.





\subsection{Visualization}
At times it is important to allow the user to do exploritory search.

\ceg{Christan will fill this in.}


